{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchcam\n",
      "  Using cached torchcam-0.4.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /home/idrone2/.local/lib/python3.10/site-packages (from torchcam) (2.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17.2 in /home/idrone2/.local/lib/python3.10/site-packages (from torchcam) (1.26.4)\n",
      "Requirement already satisfied: Pillow!=9.2.0,>=8.4.0 in /home/idrone2/.local/lib/python3.10/site-packages (from torchcam) (10.4.0)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /home/idrone2/.local/lib/python3.10/site-packages (from torchcam) (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/idrone2/.local/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/idrone2/.local/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/idrone2/.local/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/idrone2/.local/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/idrone2/.local/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/idrone2/.local/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/idrone2/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.0->torchcam) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/idrone2/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.0.0->torchcam) (12.5.82)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->torchcam) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/idrone2/.local/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.0.0->torchcam) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/idrone2/.local/lib/python3.10/site-packages (from sympy->torch<3.0.0,>=2.0.0->torchcam) (1.3.0)\n",
      "Using cached torchcam-0.4.0-py3-none-any.whl (46 kB)\n",
      "Installing collected packages: torchcam\n",
      "Successfully installed torchcam-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 18:02:37.363375: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-16 18:02:37.370712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 18:02:37.379124: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 18:02:37.381615: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 18:02:37.388205: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 18:02:37.877588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The specified directory does not exist: /home/idrone2/Tea_pest/Tea_TJ/jassid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 495\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tasks completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 398\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Verify the dataset directory exists\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(image_folder):\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified directory does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# Define valid image extensions\u001b[39;00m\n\u001b[1;32m    401\u001b[0m valid_extensions \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.bmp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tiff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The specified directory does not exist: /home/idrone2/Tea_pest/Tea_TJ/jassid"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from PIL import Image\n",
    "from torchcam.methods import GradCAM\n",
    "from torchcam.utils import overlay_mask\n",
    "import onnx\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from torch.utils.tensorboard import SummaryWriter  # For TensorBoard logging\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define the Modified U-Net Model for Classification\n",
    "# -----------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double convolution layer with BatchNorm and ReLU.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNetClassifier(nn.Module):\n",
    "    \"\"\"U-Net architecture adapted for classification.\"\"\"\n",
    "    def __init__(self, in_channels=3, num_classes=3):\n",
    "        super(UNetClassifier, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder pathway\n",
    "        x1 = self.enc1(x)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.enc2(p1)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.enc3(p2)\n",
    "        p3 = self.pool3(x3)\n",
    "        \n",
    "        x4 = self.enc4(p3)\n",
    "        p4 = self.pool4(x4)\n",
    "        \n",
    "        bottleneck = self.bottleneck(p4)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(bottleneck)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Loading and Preprocessing\n",
    "# -----------------------------\n",
    "\n",
    "def get_image_dataloader(image_folder, batch_size=16, valid_split=0.2, num_workers=4):\n",
    "    \"\"\"\n",
    "    Creates training and validation DataLoaders using ImageFolder.\n",
    "    \n",
    "    Args:\n",
    "        image_folder (str): Path to the dataset folder organized by class subdirectories.\n",
    "        batch_size (int): Batch size for DataLoaders.\n",
    "        valid_split (float): Fraction of data to be used for validation.\n",
    "        num_workers (int): Number of subprocesses for data loading.\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, class_names\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Load dataset using ImageFolder\n",
    "    dataset = datasets.ImageFolder(root=image_folder, transform=transform)\n",
    "    class_names = dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    \n",
    "    # Split dataset into training and validation sets\n",
    "    train_size = int((1 - valid_split) * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, val_loader, class_names\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define Visualization Functions\n",
    "# -----------------------------\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, save_path='training_history.png'):\n",
    "    \"\"\"Plot and save training and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved {save_path}\")\n",
    "\n",
    "def plot_confusion_matrix(model, val_loader, device, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Generate and save confusion matrix and classification report.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved {save_path}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "def visualize_predictions(model, val_loader, device, class_names, cam_extractor, num_samples=5, save_path='prediction_visualization.png'):\n",
    "    \"\"\"Visualize predictions with CAM overlays.\"\"\"\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Randomly select a batch and an image within the batch\n",
    "        inputs, labels = next(iter(val_loader))\n",
    "        idx = random.randint(0, inputs.size(0)-1)\n",
    "        input_image = inputs[idx].to(device)\n",
    "        label = labels[idx].item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_image.unsqueeze(0))\n",
    "            pred_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Get CAM\n",
    "        activation_map = cam_extractor(pred_class, output)\n",
    "        heatmap = overlay_mask(\n",
    "            image=np.uint8(255 * (input_image.cpu().permute(1, 2, 0).numpy() * np.array([0.229, 0.224, 0.225]) + \n",
    "                                      np.array([0.485, 0.456, 0.406]))),  # Denormalize\n",
    "            mask=activation_map.cpu().numpy(),\n",
    "            alpha=0.5\n",
    "        )\n",
    "        \n",
    "        # Prepare original image\n",
    "        input_np = input_image.cpu().permute(1, 2, 0).numpy()\n",
    "        # Denormalize\n",
    "        input_np = input_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        input_np = np.clip(input_np, 0, 1)\n",
    "        \n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(input_np)\n",
    "        axes[i, 0].set_title(f'Input Image\\nTrue: {class_names[label]}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot CAM overlay\n",
    "        axes[i, 1].imshow(heatmap)\n",
    "        axes[i, 1].set_title(f'Predicted: {class_names[pred_class]}')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved {save_path}\")\n",
    "\n",
    "def generate_cam_visualization(model, val_loader, device, class_names, cam_extractor, num_samples=5, save_path='cam_visualization.png'):\n",
    "    \"\"\"Generate and save CAM visualizations.\"\"\"\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(5, 5*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Randomly select a batch and an image within the batch\n",
    "        inputs, labels = next(iter(val_loader))\n",
    "        idx = random.randint(0, inputs.size(0)-1)\n",
    "        input_image = inputs[idx].to(device)\n",
    "        label = labels[idx].item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_image.unsqueeze(0))\n",
    "            pred_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Get CAM\n",
    "        activation_map = cam_extractor(pred_class, output)\n",
    "        heatmap = overlay_mask(\n",
    "            image=np.uint8(255 * (input_image.cpu().permute(1, 2, 0).numpy() * np.array([0.229, 0.224, 0.225]) + \n",
    "                                      np.array([0.485, 0.456, 0.406]))),  # Denormalize\n",
    "            mask=activation_map.cpu().numpy(),\n",
    "            alpha=0.5\n",
    "        )\n",
    "        \n",
    "        # Prepare original image\n",
    "        input_np = input_image.cpu().permute(1, 2, 0).numpy()\n",
    "        # Denormalize\n",
    "        input_np = input_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        input_np = np.clip(input_np, 0, 1)\n",
    "        \n",
    "        # Plot CAM overlay\n",
    "        axes[i].imshow(heatmap)\n",
    "        axes[i].set_title(f'Class Activation Map for {class_names[pred_class]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved {save_path}\")\n",
    "\n",
    "def generate_all_visualizations(model, train_losses, val_losses, val_loader, device, class_names):\n",
    "    \"\"\"Generate all required visualizations.\"\"\"\n",
    "    print(\"Generating training history plot...\")\n",
    "    plot_training_history(train_losses, val_losses, save_path='training_history.png')\n",
    "    \n",
    "    print(\"Generating confusion matrix and classification report...\")\n",
    "    plot_confusion_matrix(model, val_loader, device, class_names, save_path='confusion_matrix.png')\n",
    "    \n",
    "    # Initialize GradCAM extractor after generating confusion matrix\n",
    "    target_layer = 'enc4.double_conv.1'  # Adjust based on model architecture\n",
    "    cam_extractor = GradCAM(model, target_layer=target_layer)\n",
    "    \n",
    "    print(\"Generating prediction visualizations with CAM...\")\n",
    "    visualize_predictions(model, val_loader, device, class_names, cam_extractor, num_samples=5, save_path='prediction_visualization.png')\n",
    "    \n",
    "    print(\"Generating CAM visualizations...\")\n",
    "    generate_cam_visualization(model, val_loader, device, class_names, cam_extractor, num_samples=5, save_path='cam_visualization.png')\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Training Function with Progress Bar\n",
    "# -----------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=25, accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Train the model and track losses.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion (loss): Loss function.\n",
    "        optimizer (optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        accumulation_steps (int): Gradient accumulation steps.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()  # Logs will be saved to runs/ directory\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * accumulation_steps  # Multiply back\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_loss = running_loss / ((i + 1) * train_loader.batch_size)\n",
    "            progress_bar.set_postfix(loss=avg_loss)\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar('Training Loss', avg_loss, epoch * len(train_loader) + i)\n",
    "        \n",
    "        # Handle remaining gradients\n",
    "        if (i + 1) % accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}')\n",
    "        \n",
    "        # Log validation loss to TensorBoard\n",
    "        writer.add_scalar('Validation Loss', epoch_val_loss, epoch)\n",
    "        \n",
    "        # Clear cache every 5 epochs to manage GPU memory\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    generate_all_visualizations(model, train_losses, val_losses, val_loader, device, class_names)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Main Function\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # Specify the path to your dataset\n",
    "    image_folder = \"/home/idrone2/Tea_pest/Tea_TJ\"  # <-- Update this path accordingly\n",
    "    \n",
    "    # Verify the dataset directory exists\n",
    "    if not os.path.isdir(image_folder):\n",
    "        raise ValueError(f\"The specified directory does not exist: {image_folder}\")\n",
    "    \n",
    "    # Define valid image extensions\n",
    "    valid_extensions = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\", \".tiff\")\n",
    "    \n",
    "    # List all files in the directory\n",
    "    try:\n",
    "        all_files = os.listdir(image_folder)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error accessing the directory {image_folder}: {e}\")\n",
    "    \n",
    "    # Filter files with valid extensions (case-insensitive)\n",
    "    image_paths = [os.path.join(image_folder, f) for f in all_files \n",
    "                   if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    # Debugging: Print the number of images found and some sample filenames\n",
    "    print(f\"Number of images found: {len(image_paths)}\")\n",
    "    if len(image_paths) == 0:\n",
    "        print(\"Files in the directory:\")\n",
    "        for f in all_files:\n",
    "            print(f)\n",
    "        raise ValueError(\"No images found in the specified directory. Please check the path and file extensions.\")\n",
    "    \n",
    "    # Optionally, print first 5 image paths\n",
    "    print(\"Sample image paths:\")\n",
    "    for path in image_paths[:5]:\n",
    "        print(path)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader, val_loader, class_names = get_image_dataloader(image_folder=image_folder, batch_size=16, valid_split=0.2, num_workers=4)\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = UNetClassifier(in_channels=3, num_classes=num_classes).to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Identify and print all layer names to help select the correct target layer for GradCAM\n",
    "    print(\"\\nModel Layers:\")\n",
    "    for name, module in model.named_modules():\n",
    "        print(name)\n",
    "    \n",
    "    # Set the target layer for GradCAM\n",
    "    # Example: 'enc4.double_conv.1'\n",
    "    target_layer = 'enc4.double_conv.1'  # <-- Adjust based on printed layer names\n",
    "    cam_extractor = GradCAM(model, target_layer=target_layer)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    num_epochs = 25\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=num_epochs, accumulation_steps=4)\n",
    "    \n",
    "    # Save the model in PyTorch format (.pth)\n",
    "    torch.save(model.state_dict(), 'unet_classifier.pth')\n",
    "    print(\"Model saved as unet_classifier.pth\")\n",
    "    \n",
    "    # Export the model to ONNX format (.onnx)\n",
    "    dummy_input = torch.randn(1, 3, 224, 224, device=device)\n",
    "    onnx_model_path = \"unet_classifier.onnx\"\n",
    "    torch.onnx.export(model, dummy_input, onnx_model_path, \n",
    "                      input_names=['input'], output_names=['output'], \n",
    "                      dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "                      opset_version=11)\n",
    "    print(f\"Model exported to ONNX format at {onnx_model_path}\")\n",
    "    \n",
    "    # Convert ONNX model to TensorFlow SavedModel\n",
    "    tf_model_path = \"unet_classifier_tf\"\n",
    "    if not os.path.exists(tf_model_path):\n",
    "        os.makedirs(tf_model_path)\n",
    "    \n",
    "    command = f\"python -m tf2onnx.convert --onnx {onnx_model_path} --saved-model {tf_model_path}\"\n",
    "    print(f\"Converting ONNX to TensorFlow SavedModel with command: {command}\")\n",
    "    result = subprocess.run(command, shell=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"Model converted to TensorFlow SavedModel format at {tf_model_path}\")\n",
    "    else:\n",
    "        print(\"Failed to convert ONNX to TensorFlow SavedModel.\")\n",
    "    \n",
    "    # Convert TensorFlow SavedModel to TensorFlow Lite (.tflite)\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the TFLite model\n",
    "        with open(\"unet_classifier.tflite\", \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        print(\"Model converted to TensorFlow Lite format at unet_classifier.tflite\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert TensorFlow SavedModel to TFLite: {e}\")\n",
    "    \n",
    "    print(\"All tasks completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
